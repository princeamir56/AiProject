"""
===================================================================================
COMPARAISON DE 3 MOD√àLES DE MACHINE LEARNING
===================================================================================

Ce script impl√©mente et compare 3 algorithmes de ML diff√©rents pour la pr√©diction
des prix immobiliers, conform√©ment aux exigences du projet.

MOD√àLES IMPL√âMENT√âS:
1. Gradient Boosting Regressor (Ensemble - Boosting)
2. Random Forest Regressor (Ensemble - Bagging)  
3. Ridge Regression (R√©gression lin√©aire r√©gularis√©e)

JUSTIFICATION DES CHOIX:
- GradientBoosting: Excellent pour capturer les relations non-lin√©aires complexes
- RandomForest: Robuste aux outliers, bon pour la s√©lection de features
- Ridge: Mod√®le lin√©aire simple, bon baseline, g√®re la multicolin√©arit√©

AUTEUR: Projet ML - Pr√©diction de Prix Immobiliers
DATE: D√©cembre 2024
===================================================================================
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib
from datetime import datetime
from pathlib import Path
import json
import warnings
warnings.filterwarnings('ignore')

# Configuration des chemins
BASE_DIR = Path(__file__).resolve().parent.parent
DATA_PATH = BASE_DIR / "data" / "train.csv"
MODELS_DIR = Path(__file__).parent / "models"
OUTPUT_DIR = Path(__file__).parent / "comparison_outputs"
OUTPUT_DIR.mkdir(exist_ok=True)
MODELS_DIR.mkdir(exist_ok=True)

# Configuration
RANDOM_STATE = 42
TEST_SIZE = 0.2

# Features s√©lectionn√©es (bas√© sur l'analyse exploratoire)
NUMERIC_FEATURES = [
    'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',
    'TotalBsmtSF', 'GrLivArea', 'FullBath', 'HalfBath',
    'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea'
]

CATEGORICAL_FEATURES = [
    'MSZoning', 'Neighborhood', 'BldgType', 'HouseStyle',
    'CentralAir', 'KitchenQual'
]


class ModelComparer:
    """
    Classe pour comparer 3 mod√®les de ML diff√©rents
    """
    
    def __init__(self):
        self.models = {}
        self.results = {}
        self.best_model = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
    
    def load_and_prepare_data(self):
        """Charge et pr√©pare les donn√©es"""
        print("=" * 80)
        print("CHARGEMENT ET PR√âPARATION DES DONN√âES")
        print("=" * 80)
        
        df = pd.read_csv(DATA_PATH)
        print(f"\n‚úì Donn√©es charg√©es: {df.shape[0]} lignes √ó {df.shape[1]} colonnes")
        
        # Suppression des outliers identifi√©s dans l'analyse
        # (GrLivArea > 4000 et prix bas - probablement des erreurs)
        initial_count = len(df)
        df = df[~((df['GrLivArea'] > 4000) & (df['SalePrice'] < 300000))]
        print(f"‚úì Outliers supprim√©s: {initial_count - len(df)} observations")
        
        # Pr√©paration des features et target
        X = df.drop(columns=['Id', 'SalePrice'])
        y = df['SalePrice']
        
        # Split train/test
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
        )
        
        print(f"‚úì Split: {len(self.X_train)} train, {len(self.X_test)} test")
        
        return X, y
    
    def create_preprocessor(self):
        """Cr√©e le pipeline de pr√©traitement"""
        
        # Transformateur num√©rique
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])
        
        # Transformateur cat√©goriel
        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ])
        
        # Combinaison
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, NUMERIC_FEATURES),
                ('cat', categorical_transformer, CATEGORICAL_FEATURES)
            ],
            remainder='drop'
        )
        
        return preprocessor
    
    def create_models(self):
        """
        Cr√©e les 3 mod√®les avec leurs hyperparam√®tres optimis√©s
        
        JUSTIFICATION DES HYPERPARAM√àTRES:
        
        1. GradientBoosting:
           - n_estimators=200: Nombre suffisant d'arbres sans surapprentissage
           - max_depth=5: Limite la profondeur pour √©viter l'overfitting
           - learning_rate=0.1: Taux standard, bon compromis vitesse/pr√©cision
           - min_samples_split=5: √âvite les splits sur tr√®s peu d'exemples
           
        2. RandomForest:
           - n_estimators=200: Assez d'arbres pour la stabilit√©
           - max_depth=15: Plus profond car le bagging r√©gularise naturellement
           - min_samples_leaf=2: Feuilles avec au moins 2 exemples
           - max_features='sqrt': Standard pour la r√©gression
           
        3. Ridge:
           - alpha=10: R√©gularisation mod√©r√©e pour g√©rer la multicolin√©arit√©
           - Le preprocessing inclut la standardisation n√©cessaire
        """
        print("\n" + "=" * 80)
        print("CR√âATION DES MOD√àLES")
        print("=" * 80)
        
        preprocessor = self.create_preprocessor()
        
        # Mod√®le 1: Gradient Boosting (Ensemble - Boosting)
        print("\nüìä Mod√®le 1: Gradient Boosting Regressor")
        print("   Famille: Ensemble Learning (Boosting)")
        print("   Justification: Capture les relations non-lin√©aires, tr√®s performant")
        self.models['GradientBoosting'] = Pipeline([
            ('preprocessor', preprocessor),
            ('regressor', GradientBoostingRegressor(
                n_estimators=200,
                max_depth=5,
                learning_rate=0.1,
                min_samples_split=5,
                min_samples_leaf=3,
                subsample=0.8,
                random_state=RANDOM_STATE
            ))
        ])
        
        # Mod√®le 2: Random Forest (Ensemble - Bagging)
        print("\nüìä Mod√®le 2: Random Forest Regressor")
        print("   Famille: Ensemble Learning (Bagging)")
        print("   Justification: Robuste aux outliers, interpr√©table")
        self.models['RandomForest'] = Pipeline([
            ('preprocessor', preprocessor),
            ('regressor', RandomForestRegressor(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt',
                n_jobs=-1,
                random_state=RANDOM_STATE
            ))
        ])
        
        # Mod√®le 3: Ridge Regression (Lin√©aire r√©gularis√©)
        print("\nüìä Mod√®le 3: Ridge Regression")
        print("   Famille: R√©gression lin√©aire r√©gularis√©e (L2)")
        print("   Justification: Simple, interpr√©table, g√®re la multicolin√©arit√©")
        self.models['Ridge'] = Pipeline([
            ('preprocessor', preprocessor),
            ('regressor', Ridge(alpha=10, random_state=RANDOM_STATE))
        ])
        
        return self.models
    
    def train_and_evaluate(self):
        """Entra√Æne et √©value tous les mod√®les"""
        print("\n" + "=" * 80)
        print("ENTRA√éNEMENT ET √âVALUATION DES MOD√àLES")
        print("=" * 80)
        
        for name, model in self.models.items():
            print(f"\n{'‚îÄ' * 60}")
            print(f"üìà Training: {name}")
            print(f"{'‚îÄ' * 60}")
            
            # Entra√Ænement
            model.fit(self.X_train, self.y_train)
            
            # Pr√©dictions
            y_train_pred = model.predict(self.X_train)
            y_test_pred = model.predict(self.X_test)
            
            # M√©triques
            train_rmse = np.sqrt(mean_squared_error(self.y_train, y_train_pred))
            test_rmse = np.sqrt(mean_squared_error(self.y_test, y_test_pred))
            train_mae = mean_absolute_error(self.y_train, y_train_pred)
            test_mae = mean_absolute_error(self.y_test, y_test_pred)
            train_r2 = r2_score(self.y_train, y_train_pred)
            test_r2 = r2_score(self.y_test, y_test_pred)
            
            # Cross-validation
            cv_scores = cross_val_score(model, self.X_train, self.y_train, 
                                        cv=5, scoring='neg_root_mean_squared_error')
            cv_rmse = -cv_scores.mean()
            cv_std = cv_scores.std()
            
            # Stocker les r√©sultats
            self.results[name] = {
                'train_rmse': train_rmse,
                'test_rmse': test_rmse,
                'train_mae': train_mae,
                'test_mae': test_mae,
                'train_r2': train_r2,
                'test_r2': test_r2,
                'cv_rmse': cv_rmse,
                'cv_std': cv_std
            }
            
            print(f"\n   üìä R√©sultats:")
            print(f"   ‚îú‚îÄ Train RMSE: ${train_rmse:,.0f}")
            print(f"   ‚îú‚îÄ Test RMSE:  ${test_rmse:,.0f}")
            print(f"   ‚îú‚îÄ Train R¬≤:   {train_r2:.4f}")
            print(f"   ‚îú‚îÄ Test R¬≤:    {test_r2:.4f}")
            print(f"   ‚îî‚îÄ CV RMSE:    ${cv_rmse:,.0f} (¬±${cv_std:,.0f})")
        
        return self.results
    
    def compare_models(self):
        """Compare et visualise les performances des mod√®les"""
        print("\n" + "=" * 80)
        print("COMPARAISON DES MOD√àLES")
        print("=" * 80)
        
        # Cr√©er un DataFrame de comparaison
        comparison_df = pd.DataFrame(self.results).T
        comparison_df['Overfitting'] = comparison_df['train_rmse'] - comparison_df['test_rmse']
        
        print("\nüìä TABLEAU COMPARATIF:")
        print("‚îÄ" * 90)
        print(f"{'Mod√®le':<20} {'Train RMSE':>12} {'Test RMSE':>12} {'Test R¬≤':>10} {'CV RMSE':>12} {'Overfit':>10}")
        print("‚îÄ" * 90)
        
        for name, metrics in self.results.items():
            overfit = metrics['train_rmse'] - metrics['test_rmse']
            print(f"{name:<20} ${metrics['train_rmse']:>10,.0f} ${metrics['test_rmse']:>10,.0f} "
                  f"{metrics['test_r2']:>10.4f} ${metrics['cv_rmse']:>10,.0f} ${overfit:>9,.0f}")
        
        print("‚îÄ" * 90)
        
        # Trouver le meilleur mod√®le
        best_name = min(self.results, key=lambda x: self.results[x]['test_rmse'])
        self.best_model = self.models[best_name]
        
        print(f"\nüèÜ MEILLEUR MOD√àLE: {best_name}")
        print(f"   ‚îî‚îÄ Test RMSE: ${self.results[best_name]['test_rmse']:,.0f}")
        print(f"   ‚îî‚îÄ Test R¬≤: {self.results[best_name]['test_r2']:.4f}")
        
        # Visualisations
        self._plot_comparison()
        self._plot_predictions()
        self._plot_feature_importance()
        
        return best_name, self.results
    
    def _plot_comparison(self):
        """G√©n√®re les graphiques de comparaison"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        models = list(self.results.keys())
        colors = ['#2563eb', '#10b981', '#f59e0b']
        
        # RMSE Comparison
        train_rmse = [self.results[m]['train_rmse'] for m in models]
        test_rmse = [self.results[m]['test_rmse'] for m in models]
        
        x = np.arange(len(models))
        width = 0.35
        
        axes[0].bar(x - width/2, train_rmse, width, label='Train', color='lightblue')
        axes[0].bar(x + width/2, test_rmse, width, label='Test', color=colors)
        axes[0].set_ylabel('RMSE ($)')
        axes[0].set_title('Comparaison RMSE', fontweight='bold')
        axes[0].set_xticks(x)
        axes[0].set_xticklabels(models, rotation=15)
        axes[0].legend()
        
        # R¬≤ Comparison
        train_r2 = [self.results[m]['train_r2'] for m in models]
        test_r2 = [self.results[m]['test_r2'] for m in models]
        
        axes[1].bar(x - width/2, train_r2, width, label='Train', color='lightgreen')
        axes[1].bar(x + width/2, test_r2, width, label='Test', color=colors)
        axes[1].set_ylabel('R¬≤ Score')
        axes[1].set_title('Comparaison R¬≤', fontweight='bold')
        axes[1].set_xticks(x)
        axes[1].set_xticklabels(models, rotation=15)
        axes[1].legend()
        axes[1].set_ylim(0.7, 1.0)
        
        # CV RMSE with error bars
        cv_rmse = [self.results[m]['cv_rmse'] for m in models]
        cv_std = [self.results[m]['cv_std'] for m in models]
        
        axes[2].bar(models, cv_rmse, color=colors, yerr=cv_std, capsize=5)
        axes[2].set_ylabel('CV RMSE ($)')
        axes[2].set_title('Cross-Validation RMSE (¬±std)', fontweight='bold')
        axes[2].tick_params(axis='x', rotation=15)
        
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / 'model_comparison.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"\n‚úì Graphique sauvegard√©: {OUTPUT_DIR / 'model_comparison.png'}")
    
    def _plot_predictions(self):
        """Graphique des pr√©dictions vs valeurs r√©elles"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        for i, (name, model) in enumerate(self.models.items()):
            y_pred = model.predict(self.X_test)
            
            axes[i].scatter(self.y_test, y_pred, alpha=0.5, c='#2563eb')
            
            # Ligne parfaite
            min_val = min(self.y_test.min(), y_pred.min())
            max_val = max(self.y_test.max(), y_pred.max())
            axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)
            
            axes[i].set_xlabel('Prix r√©el ($)')
            axes[i].set_ylabel('Prix pr√©dit ($)')
            axes[i].set_title(f'{name}\nR¬≤ = {self.results[name]["test_r2"]:.4f}', fontweight='bold')
        
        plt.suptitle('Pr√©dictions vs Valeurs R√©elles', fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / 'predictions_scatter.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"‚úì Graphique sauvegard√©: {OUTPUT_DIR / 'predictions_scatter.png'}")
    
    def _plot_feature_importance(self):
        """Graphique d'importance des features (pour GB et RF)"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        for i, name in enumerate(['GradientBoosting', 'RandomForest']):
            model = self.models[name]
            regressor = model.named_steps['regressor']
            preprocessor = model.named_steps['preprocessor']
            
            # Obtenir les noms de features apr√®s transformation
            feature_names = NUMERIC_FEATURES.copy()
            if hasattr(preprocessor.named_transformers_['cat'].named_steps['encoder'], 'get_feature_names_out'):
                cat_features = preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(CATEGORICAL_FEATURES)
                feature_names.extend(cat_features)
            
            importances = regressor.feature_importances_
            
            # Top 15 features
            indices = np.argsort(importances)[-15:]
            
            axes[i].barh(range(len(indices)), importances[indices], color='#2563eb')
            axes[i].set_yticks(range(len(indices)))
            
            # Raccourcir les noms si n√©cessaire
            labels = [feature_names[j] if j < len(feature_names) else f'Feature {j}' for j in indices]
            labels = [l[:20] + '...' if len(l) > 20 else l for l in labels]
            axes[i].set_yticklabels(labels)
            axes[i].set_xlabel('Importance')
            axes[i].set_title(f'Feature Importance - {name}', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(OUTPUT_DIR / 'feature_importance.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"‚úì Graphique sauvegard√©: {OUTPUT_DIR / 'feature_importance.png'}")
    
    def save_best_model(self):
        """Sauvegarde le meilleur mod√®le"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = MODELS_DIR / f"best_model_{timestamp}.joblib"
        
        joblib.dump(self.best_model, model_path)
        
        # Sauvegarder aussi les m√©tadonn√©es
        best_name = min(self.results, key=lambda x: self.results[x]['test_rmse'])
        metadata = {
            'model_name': best_name,
            'version': timestamp,
            'metrics': self.results[best_name],
            'features': {
                'numeric': NUMERIC_FEATURES,
                'categorical': CATEGORICAL_FEATURES
            }
        }
        
        with open(MODELS_DIR / f"best_model_{timestamp}_metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"\n‚úì Mod√®le sauvegard√©: {model_path}")
        
        return model_path
    
    def generate_report(self):
        """G√©n√®re un rapport complet"""
        report_path = OUTPUT_DIR / 'comparison_report.txt'
        
        best_name = min(self.results, key=lambda x: self.results[x]['test_rmse'])
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("RAPPORT DE COMPARAISON DES MOD√àLES DE MACHINE LEARNING\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("MOD√àLES IMPL√âMENT√âS\n")
            f.write("-" * 40 + "\n\n")
            
            f.write("1. GRADIENT BOOSTING REGRESSOR\n")
            f.write("   Famille: Ensemble Learning (Boosting)\n")
            f.write("   Hyperparam√®tres:\n")
            f.write("     - n_estimators: 200\n")
            f.write("     - max_depth: 5\n")
            f.write("     - learning_rate: 0.1\n")
            f.write("   Justification: M√©thode d'ensemble qui construit s√©quentiellement\n")
            f.write("   des arbres de d√©cision, chacun corrigeant les erreurs du pr√©c√©dent.\n")
            f.write("   Excellent pour capturer les relations non-lin√©aires complexes.\n\n")
            
            f.write("2. RANDOM FOREST REGRESSOR\n")
            f.write("   Famille: Ensemble Learning (Bagging)\n")
            f.write("   Hyperparam√®tres:\n")
            f.write("     - n_estimators: 200\n")
            f.write("     - max_depth: 15\n")
            f.write("     - max_features: sqrt\n")
            f.write("   Justification: Agr√®ge les pr√©dictions de multiples arbres ind√©pendants.\n")
            f.write("   Robuste aux outliers et fournit une mesure d'importance des features.\n\n")
            
            f.write("3. RIDGE REGRESSION\n")
            f.write("   Famille: R√©gression lin√©aire r√©gularis√©e (L2)\n")
            f.write("   Hyperparam√®tres:\n")
            f.write("     - alpha: 10\n")
            f.write("   Justification: Mod√®le lin√©aire simple avec r√©gularisation L2.\n")
            f.write("   G√®re bien la multicolin√©arit√© et sert de baseline interpr√©table.\n\n")
            
            f.write("\nR√âSULTATS\n")
            f.write("-" * 40 + "\n\n")
            
            for name, metrics in self.results.items():
                f.write(f"{name}:\n")
                f.write(f"  - Test RMSE: ${metrics['test_rmse']:,.0f}\n")
                f.write(f"  - Test R¬≤: {metrics['test_r2']:.4f}\n")
                f.write(f"  - CV RMSE: ${metrics['cv_rmse']:,.0f} (¬±${metrics['cv_std']:,.0f})\n\n")
            
            f.write(f"\nüèÜ MEILLEUR MOD√àLE RETENU: {best_name}\n")
            f.write("-" * 40 + "\n")
            f.write(f"Ce mod√®le a √©t√© s√©lectionn√© car il pr√©sente:\n")
            f.write(f"  - Le meilleur RMSE sur les donn√©es de test\n")
            f.write(f"  - Un bon score R¬≤ ({self.results[best_name]['test_r2']:.4f})\n")
            f.write(f"  - Une bonne g√©n√©ralisation (faible overfitting)\n")
        
        print(f"‚úì Rapport sauvegard√©: {report_path}")


def main():
    """Ex√©cute la comparaison compl√®te des mod√®les"""
    print("\n" + "‚ñà" * 80)
    print("  COMPARAISON DE 3 MOD√àLES DE MACHINE LEARNING")
    print("‚ñà" * 80)
    
    # Initialiser le comparateur
    comparer = ModelComparer()
    
    # 1. Charger les donn√©es
    comparer.load_and_prepare_data()
    
    # 2. Cr√©er les mod√®les
    comparer.create_models()
    
    # 3. Entra√Æner et √©valuer
    comparer.train_and_evaluate()
    
    # 4. Comparer les mod√®les
    best_name, results = comparer.compare_models()
    
    # 5. Sauvegarder le meilleur mod√®le
    comparer.save_best_model()
    
    # 6. G√©n√©rer le rapport
    comparer.generate_report()
    
    print("\n" + "=" * 80)
    print("‚úÖ COMPARAISON TERMIN√âE")
    print("=" * 80)
    print(f"\nüìÅ Fichiers g√©n√©r√©s dans: {OUTPUT_DIR}")
    print("   - model_comparison.png")
    print("   - predictions_scatter.png")
    print("   - feature_importance.png")
    print("   - comparison_report.txt")
    
    return results


if __name__ == "__main__":
    main()
